"""
Full analysis script for the article:
"Gender Disparities in Random Blood Glucose Levels Among Pakistani Adults with Type 2 Diabetes"
This script reproduces all statistical and machine learning analyses reported in the paper.
It uses the synthetic dataset 'diabetes_study_raw.csv' (provided separately).
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.model_selection import (train_test_split, cross_val_score,
                                     KFold, GridSearchCV)
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.svm import SVR
from sklearn.ensemble import RandomForestRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import mean_absolute_error, r2_score
import warnings
warnings.filterwarnings('ignore')

# ---------------------------
# 1. Load the synthetic dataset
# ---------------------------
print("Loading dataset...")
df = pd.read_csv("C:\\Users\\Dell\\Downloads\\diabetic_csv_20260223_6645e9.txt")
print(f"Dataset shape: {df.shape}")
print(df.head())

# ---------------------------
# 2. Data Preprocessing for Modeling
# ---------------------------
target = 'rbs'
categorical_features = ['activity', 'ses', 'medication']
binary_features = ['gender', 'family_history', 'fasting', 'hypertension', 'cvd', 'dyslipidemia']
continuous_features = ['age', 'bmi', 'duration']

# Create preprocessing pipeline
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), continuous_features),
        ('bin', 'passthrough', binary_features),  # no scaling for binary
        ('cat', OneHotEncoder(drop='first'), categorical_features)
    ])

# Prepare feature matrix X and target y
X = df.drop(columns=[target] + ['comorbidities'])  # comorbidities is redundant
y = df[target]

# Split into train/test (80/20)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ---------------------------
# 3. Simple Linear Regression (Gender Only)
# ---------------------------
print("\n--- Simple Linear Regression (gender only) ---")
X_gender = df[['gender']].values
lr_gender = LinearRegression()
lr_gender.fit(X_gender, y)
print(f"Intercept: {lr_gender.intercept_:.2f}")
print(f"Coefficient (gender): {lr_gender.coef_[0]:.2f}")
print(f"R²: {r2_score(y, lr_gender.predict(X_gender)):.3f}")

# ---------------------------
# 4. Multivariate Linear Regression
# ---------------------------
print("\n--- Multivariate Linear Regression ---")
# Preprocess full feature set
X_train_proc = preprocessor.fit_transform(X_train)
X_test_proc = preprocessor.transform(X_test)

lr_full = LinearRegression()
lr_full.fit(X_train_proc, y_train)
y_pred_lr = lr_full.predict(X_test_proc)
print(f"Test R²: {r2_score(y_test, y_pred_lr):.3f}")
print(f"Test MAE: {mean_absolute_error(y_test, y_pred_lr):.2f} mg/dL")

# Extract coefficients (with feature names)
feature_names = (continuous_features +
                 binary_features +
                 list(preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features)))
coef_df = pd.DataFrame({'feature': feature_names, 'coefficient': lr_full.coef_})
print("\nMultivariate Coefficients:")
print(coef_df.sort_values('coefficient', ascending=False).head(10).to_string(index=False))

# ---------------------------
# 5. Diagnostic Plots (for gender-only model)
# ---------------------------
residuals = y - lr_gender.predict(X_gender)
fitted = lr_gender.predict(X_gender)

fig, axes = plt.subplots(1, 3, figsize=(15, 4))

# Residuals vs fitted
axes[0].scatter(fitted, residuals, alpha=0.5)
axes[0].axhline(y=0, color='r', linestyle='--')
axes[0].set_xlabel('Fitted values (mg/dL)')
axes[0].set_ylabel('Residuals')
axes[0].set_title('Residuals vs Fitted')

# Histogram of residuals
axes[1].hist(residuals, bins=20, edgecolor='black', density=True)
axes[1].set_xlabel('Residuals')
axes[1].set_ylabel('Density')
axes[1].set_title('Distribution of Residuals')
# Overlay normal curve
x_norm = np.linspace(residuals.min(), residuals.max(), 100)
axes[1].plot(x_norm, stats.norm.pdf(x_norm, residuals.mean(), residuals.std()), 'r-', label='Normal')

# Q-Q plot
stats.probplot(residuals, dist="norm", plot=axes[2])
axes[2].set_title('Q-Q Plot')

plt.tight_layout()
plt.savefig('diagnostic_plots.png', dpi=150)
plt.show()

# Shapiro-Wilk test
shapiro_stat, shapiro_p = stats.shapiro(residuals)
print(f"\nShapiro-Wilk test: statistic={shapiro_stat:.3f}, p={shapiro_p:.3f}")

# Breusch-Pagan test (using statsmodels)
import statsmodels.api as sm
from statsmodels.stats.diagnostic import het_breuschpagan
X_gender_sm = sm.add_constant(X_gender)
model_sm = sm.OLS(y, X_gender_sm).fit()
bp_test = het_breuschpagan(model_sm.resid, X_gender_sm)
print(f"Breusch-Pagan test: LM={bp_test[0]:.3f}, p={bp_test[1]:.3f}")

# ---------------------------
# 6. Machine Learning Models with Cross-Validation
# ---------------------------
print("\n--- Machine Learning Models ---")
models = {
    'Ridge': Ridge(random_state=42),
    'SVR': SVR(),
    'Random Forest': RandomForestRegressor(random_state=42),
    'Neural Network': MLPRegressor(random_state=42, max_iter=500),
    'Polynomial Regression (degree=2)': Pipeline([
        ('poly', PolynomialFeatures(degree=2, include_bias=False)),
        ('lin', LinearRegression())
    ])
}

# Parameter grids for tuning
param_grids = {
    'Ridge': {'alpha': [0.1, 1, 10]},
    'SVR': {'C': [0.1, 1, 10], 'gamma': ['scale', 'auto']},
    'Random Forest': {'n_estimators': [50, 100, 200], 'max_depth': [3, 5, 10]},
    'Neural Network': {'hidden_layer_sizes': [(50,), (100,), (50,50)], 'alpha': [0.0001, 0.001]},
    'Polynomial Regression (degree=2)': {}  # no tuning
}

# Use the same preprocessed data for ML
scaler = StandardScaler()
X_train_ml = scaler.fit_transform(X_train_proc)  # already preprocessed, but scale again
X_test_ml = scaler.transform(X_test_proc)

results = []
best_models = {}

for name, model in models.items():
    print(f"\nTraining {name}...")
    if param_grids[name]:
        # Simple grid search with 5-fold CV
        gs = GridSearchCV(model, param_grids[name], cv=5, scoring='neg_mean_absolute_error', n_jobs=-1)
        gs.fit(X_train_ml, y_train)
        best_model = gs.best_estimator_
        print(f"Best params: {gs.best_params_}")
    else:
        best_model = model.fit(X_train_ml, y_train)

    # Predict and evaluate
    y_pred = best_model.predict(X_test_ml)
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    results.append({
        'Model': name,
        'MAE (mg/dL)': f"{mae:.2f}",
        'R²': f"{r2:.3f}"
    })
    best_models[name] = best_model

results_df = pd.DataFrame(results)
print("\nModel Performance on Test Set:")
print(results_df.to_string(index=False))

# ---------------------------
# 7. Feature Importance (Random Forest)
# ---------------------------
rf_model = best_models['Random Forest']
importances = rf_model.feature_importances_
# Get feature names after preprocessing
feature_names = (continuous_features +
                 binary_features +
                 list(preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features)))
importance_df = pd.DataFrame({'feature': feature_names, 'importance': importances})
importance_df = importance_df.sort_values('importance', ascending=False).head(10)
print("\nTop 10 Feature Importances (Random Forest):")
print(importance_df.to_string(index=False))

# Plot feature importance
plt.figure(figsize=(8,5))
sns.barplot(data=importance_df, x='importance', y='feature')
plt.title('Random Forest Feature Importance')
plt.tight_layout()
plt.savefig('feature_importance.png', dpi=150)
plt.show()

# ---------------------------
# 8. Age-Stratified Analysis
# ---------------------------
print("\n--- Age-Stratified Analysis ---")
age_bins = [35, 45, 55, 61]
age_labels = ['35-45', '46-55', '56-60']
df['age_group'] = pd.cut(df['age'], bins=age_bins, labels=age_labels, right=False)

age_stats = df.groupby('age_group')['rbs'].agg(['mean', 'std', 'count'])
age_stats['mean_ci_lower'] = age_stats['mean'] - 1.96 * age_stats['std'] / np.sqrt(age_stats['count'])
age_stats['mean_ci_upper'] = age_stats['mean'] + 1.96 * age_stats['std'] / np.sqrt(age_stats['count'])
print(age_stats[['mean', 'mean_ci_lower', 'mean_ci_upper']])

# Pearson correlation
corr, p_val = stats.pearsonr(df['age'], df['rbs'])
print(f"\nPearson correlation age vs RBS: r={corr:.3f}, p={p_val:.3f}")

# ---------------------------
# 9. Gender-Based Distribution (like Figure 2)
# ---------------------------
print("\n--- Gender-Based Distribution ---")
gender_labels = {0: 'Male', 1: 'Female'}
df['gender_label'] = df['gender'].map(gender_labels)
bins = [70, 160, 260, 400]
labels = ['Normal', 'Abnormal', 'Severe']
df['glucose_category'] = pd.cut(df['rbs'], bins=bins, labels=labels, right=False)
gender_counts = df.groupby(['gender_label', 'glucose_category']).size().unstack()
print(gender_counts)

# Plot
fig, axes = plt.subplots(1, 2, figsize=(12,4))
# Left: stacked bar
gender_counts.T.plot(kind='bar', ax=axes[0], color=['green', 'orange', 'red'])
axes[0].set_title('Blood Sugar Levels by Gender')
axes[0].set_ylabel('Count')
axes[0].set_xlabel('Category')
axes[0].legend(title='Gender')

# Right: pie of gender
df['gender_label'].value_counts().plot(kind='pie', ax=axes[1], autopct='%1.1f%%', colors=['lightblue', 'pink'])
axes[1].set_ylabel('')
axes[1].set_title('Gender Distribution')
plt.tight_layout()
plt.savefig('gender_distribution.png', dpi=150)
plt.show()

# ---------------------------
# 10. Save Results Summary
# ---------------------------
with open('analysis_results.txt', 'w') as f:
    f.write("=== Diabetes Study Analysis Results ===\n\n")
    f.write("1. Simple Linear Regression (gender only):\n")
    f.write(f"   R² = {r2_score(y, lr_gender.predict(X_gender)):.3f}\n")
    f.write(f"   MAE = {mean_absolute_error(y, lr_gender.predict(X_gender)):.2f} mg/dL\n\n")
    f.write("2. Multivariate Linear Regression:\n")
    f.write(f"   Test R² = {r2_score(y_test, y_pred_lr):.3f}\n")
    f.write(f"   Test MAE = {mean_absolute_error(y_test, y_pred_lr):.2f} mg/dL\n\n")
    f.write("3. Machine Learning Model Performance:\n")
    f.write(results_df.to_string(index=False) + "\n\n")
    f.write("4. Age-Stratified Means:\n")
    f.write(age_stats[['mean', 'mean_ci_lower', 'mean_ci_upper']].to_string() + "\n\n")
    f.write("5. Gender Distribution:\n")
    f.write(gender_counts.to_string() + "\n")

print("\nAll analyses completed. Results saved to 'analysis_results.txt' and figures saved as PNG.")
