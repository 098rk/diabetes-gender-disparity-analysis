import numpy as np
import pandas as pd
import statsmodels.api as sm
from scipy import stats
from sklearn.model_selection import KFold, GridSearchCV, cross_val_score
from sklearn.linear_model import Ridge
from sklearn.svm import SVR
from sklearn.ensemble import RandomForestRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_absolute_error, r2_score
from sklearn.inspection import permutation_importance
import warnings

warnings.filterwarnings('ignore')

# ------------------------------------------------------------
# 1. Generate or load synthetic dataset
# ------------------------------------------------------------
np.random.seed(42)  # Ensures reproducibility
n = 300

# Gender: 0 = male, 1 = female (balanced)
gender = np.array([0] * 150 + [1] * 150)
np.random.shuffle(gender)

# Age: uniform between 35 and 60
age = np.random.uniform(35, 60, n)

# BMI with gender-specific means
bmi = np.zeros(n)
bmi[gender == 0] = np.random.normal(27, 4, (gender == 0).sum())
bmi[gender == 1] = np.random.normal(29, 4, (gender == 1).sum())
bmi = np.clip(bmi, 18, 40)

# Physical activity (categorical)
activity = np.empty(n, dtype=object)
activity[gender == 0] = np.random.choice(['Low', 'Moderate', 'High'],
                                         size=(gender == 0).sum(),
                                         p=[0.2, 0.5, 0.3])
activity[gender == 1] = np.random.choice(['Low', 'Moderate', 'High'],
                                         size=(gender == 1).sum(),
                                         p=[0.55, 0.25, 0.2])

# Socioeconomic status (categorical)
ses = np.empty(n, dtype=object)
ses[gender == 0] = np.random.choice(['Low', 'Middle', 'High'],
                                    size=(gender == 0).sum(),
                                    p=[0.3, 0.5, 0.2])
ses[gender == 1] = np.random.choice(['Low', 'Middle', 'High'],
                                    size=(gender == 1).sum(),
                                    p=[0.5, 0.3, 0.2])

# Family history of diabetes (binary)
famhist = np.zeros(n)
famhist[gender == 0] = np.random.binomial(1, 0.35, (gender == 0).sum())
famhist[gender == 1] = np.random.binomial(1, 0.25, (gender == 1).sum())

# Coefficients tuned to approximate the article's numbers
intercept = 50.0
age_coef = 2.8
gender_coef = 25.0
bmi_coef = 1.5
famhist_coef = -15.0
activity_map = {'Low': 0, 'Moderate': -5, 'High': -10}
ses_map = {'Low': 0, 'Middle': -4, 'High': -8}

# Linear predictor
pred = (intercept +
        age_coef * age +
        gender_coef * gender +
        bmi_coef * bmi +
        famhist_coef * famhist +
        np.array([activity_map[a] for a in activity]) +
        np.array([ses_map[s] for s in ses]))

# Add noise
noise_sd = 30
rbs = pred + np.random.normal(0, noise_sd, n)
rbs = np.maximum(rbs, 50)

# Create DataFrame
df = pd.DataFrame({
    'gender': gender,
    'age': age,
    'bmi': bmi,
    'activity': activity,
    'ses': ses,
    'family_history': famhist,
    'rbs': rbs
})

# Save for reproducibility
df.to_csv('diabetes_synthetic.csv', index=False)
print("Synthetic dataset saved as 'diabetes_synthetic.csv'")
print(f"Dataset shape: {df.shape}\n")

# ------------------------------------------------------------
# 2. Descriptive statistics
# ------------------------------------------------------------
print("--- Descriptive Statistics ---")
print("Gender distribution:")
print(df['gender'].value_counts().rename(index={0: 'Male', 1: 'Female'}))
print("\nMean RBS by gender:")
print(df.groupby('gender')['rbs'].mean().rename(index={0: 'Male', 1: 'Female'}))

def glucose_category(glucose):
    if glucose < 160:
        return 'Normal'
    elif glucose < 260:
        return 'Abnormal'
    else:
        return 'Severe'

df['glucose_cat'] = df['rbs'].apply(glucose_category)
print("\nGlycemic distribution (overall):")
print(df['glucose_cat'].value_counts())
print("\nGlycemic distribution by gender:")
print(pd.crosstab(df['gender'], df['glucose_cat']).rename(index={0: 'Male', 1: 'Female'}))

# ------------------------------------------------------------
# 3. Linear regression models
# ------------------------------------------------------------
print("\n--- Linear Regression Models ---")

# Simple linear (gender only)
X_gender = sm.add_constant(df['gender'])
model_gender = sm.OLS(df['rbs'], X_gender).fit()
print("\nSimple Linear Regression (gender only):")
print(f"Intercept: {model_gender.params[0]:.2f}")
print(f"Coefficient (gender): {model_gender.params[1]:.2f}")
print(f"R²: {model_gender.rsquared:.3f}")

# Bivariate age
X_age = sm.add_constant(df['age'])
model_age = sm.OLS(df['rbs'], X_age).fit()
print("\nBivariate Regression (age only):")
print(f"Intercept: {model_age.params[0]:.2f}")
print(f"Coefficient (age): {model_age.params[1]:.2f}")
print(f"R²: {model_age.rsquared:.3f}")

# Multivariate (all demographics)
df_dummies = pd.get_dummies(df, columns=['activity', 'ses'], drop_first=True)
for col in df_dummies.select_dtypes(include=['bool', 'uint8']).columns:
    df_dummies[col] = df_dummies[col].astype(float)

feature_cols = ['gender', 'age', 'bmi', 'family_history'] + \
               [c for c in df_dummies.columns if 'activity_' in c or 'ses_' in c]
X = df_dummies[feature_cols].astype(float)
X = sm.add_constant(X)
y = df_dummies['rbs'].astype(float)

model_multi = sm.OLS(y, X).fit()
print("\nMultivariate Linear Regression (all demographics):")
print(model_multi.summary().tables[1])
print(f"R²: {model_multi.rsquared:.3f}")

# Age-stratified means
df['age_group'] = pd.cut(df['age'], bins=[35, 45, 55, 60], labels=['35-45', '46-55', '56-60'])
age_means = df.groupby('age_group')['rbs'].agg(['mean',
                                                lambda x: x.mean() - 1.96 * x.std() / np.sqrt(len(x)),
                                                lambda x: x.mean() + 1.96 * x.std() / np.sqrt(len(x))])
age_means.columns = ['mean', 'ci_lower', 'ci_upper']
print("\nAge-Stratified Mean RBS (with 95% CI):")
print(age_means)

# Pearson correlation age vs RBS
corr, pval = stats.pearsonr(df['age'], df['rbs'])
print(f"\nPearson correlation age vs RBS: r={corr:.3f}, p={pval:.3f}")

# ------------------------------------------------------------
# 4. Machine Learning models with NESTED cross-validation
# ------------------------------------------------------------
print("\n--- Machine Learning Models (Nested CV) ---")

# Prepare features and target (same as multivariate, without constant)
X_ml = df_dummies[feature_cols].astype(float)
y_ml = df['rbs'].astype(float)

# Scale features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_ml)

# Outer CV (5-fold) for unbiased performance estimate
outer_cv = KFold(n_splits=5, shuffle=True, random_state=42)

# Define models and parameter grids
models = {
    'Ridge': Ridge(),
    'SVR': SVR(),
    'Random Forest': RandomForestRegressor(random_state=42),
    'Neural Network': MLPRegressor(max_iter=1000, random_state=42),
    'Polynomial Regression (degree=2)': Pipeline([
        ('poly', PolynomialFeatures(degree=2, include_bias=False)),
        ('ridge', Ridge())
    ])
}

param_grids = {
    'Ridge': {'alpha': [0.1, 1, 10]},
    'SVR': {'C': [0.1, 1, 10], 'gamma': ['scale', 'auto']},
    'Random Forest': {'n_estimators': [50, 100, 200], 'max_depth': [3, 5, 10]},
    'Neural Network': {'hidden_layer_sizes': [(50,), (100,), (50, 50)], 'alpha': [0.0001, 0.001, 0.01]},
    'Polynomial Regression (degree=2)': {'ridge__alpha': [0.1, 1, 10]}
}

nested_results = []

for name, model in models.items():
    print(f"\nEvaluating {name} with nested CV...")
    fold_scores_r2 = []
    fold_scores_mae = []
    best_models_per_fold = []  # store best model from each inner fold

    for train_idx, test_idx in outer_cv.split(X_scaled, y_ml):
        X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]
        y_train, y_test = y_ml.iloc[train_idx], y_ml.iloc[test_idx]

        # Inner CV for hyperparameter tuning (3-fold)
        inner_cv = KFold(n_splits=3, shuffle=True, random_state=42)
        grid = GridSearchCV(model, param_grids[name], cv=inner_cv, scoring='r2', n_jobs=-1)
        grid.fit(X_train, y_train)
        best_model = grid.best_estimator_
        best_models_per_fold.append(best_model)

        # Evaluate on outer test fold
        y_pred = best_model.predict(X_test)
        fold_scores_r2.append(r2_score(y_test, y_pred))
        fold_scores_mae.append(mean_absolute_error(y_test, y_pred))

    # Aggregate results
    mean_r2 = np.mean(fold_scores_r2)
    mean_mae = np.mean(fold_scores_mae)
    nested_results.append({
        'Model': name,
        'MAE (mg/dL)': round(mean_mae, 2),
        'R²': round(mean_r2, 3)
    })
    print(f"  MAE: {mean_mae:.2f} mg/dL, R²: {mean_r2:.3f}")

    # For Random Forest, extract feature importance from the first outer fold's best model
    if name == 'Random Forest':
        rf_best = best_models_per_fold[0]
        importances = rf_best.feature_importances_
        feature_names = feature_cols
        imp_df = pd.DataFrame({'feature': feature_names, 'importance': importances})
        imp_df = imp_df.sort_values('importance', ascending=False)
        print("\nTop 5 feature importances (Random Forest, first outer fold):")
        print(imp_df.head(5).to_string(index=False))

# Summary table
nested_results_df = pd.DataFrame(nested_results)
print("\nModel Performance Summary (nested CV):")
print(nested_results_df.to_string(index=False))

# ------------------------------------------------------------
# 5. Diagnostic tests (on multivariate model)
# ------------------------------------------------------------
print("\n--- Diagnostic Tests ---")
residuals = model_multi.resid
shapiro_stat, shapiro_p = stats.shapiro(residuals)
print(f"Shapiro-Wilk test: statistic={shapiro_stat:.3f}, p={shapiro_p:.3f}")

from statsmodels.stats.diagnostic import het_breuschpagan
bp_test = het_breuschpagan(residuals, model_multi.model.exog)
print(f"Breusch-Pagan test: LM={bp_test[0]:.3f}, p={bp_test[1]:.3f}")

print("\nAll analyses completed.")
